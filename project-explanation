Our goal for this project is to solve the Vehicle Routing Problem(https://en.wikipedia.org/wiki/Vehicle_routing_problem). The problem states: If you need to travel a certain number of points/locations, and you have multiple vehicles, what is the most optimal routes to traverse the points. Such a problem is a NP Hard(https://en.wikipedia.org/wiki/NP-hardness), i.e. it's difficulty scales exponentially. What we aim to do is to optimize this as much as possible using Reinforcement Learning.
Reinforcement Learning is a kind of machine learning where you train the program without knowing the correct solution, but the developer tells the model what is good and what is bad. For eg, in a game of football, where the player is our model/agent, we can tell the program to shoot the ball in the direction of the goal and to NOT pass the ball near an enemy player. We do this by assigning a Reward to each of these outcomes. Based on the surrounding / "observation" of the agent, the agent picks the action which gives the most total reward it gets over time, known as "return". The reward reduces as the agent thinks about future time steps as it is not certain what will actually happen.
Note: I often to this problem as a game, with the model basically playing it for us

They are 2 major structure choices via which the agent learns
1. Use Temporal Learning (Learning with Time):
When the agent starts, it doesn't know what the best action would be, as it doesn't know how the observation will change if it does a particular action. We let the agent play the whole game, with it randomly picking its moves. Now, we know the next observation at any particular time is and can calculate the next reward. This allows us to calculate the expected return by simply stating the Return = Next_Reward + Return_after_this_action*reducing_number (reducing number as we aren't certain it's right). Notice the right hand side, is more correct (though marginally) and we nudge our model towards this more correct approximation again and again, we get a function that approximates the expected return very well, and we can then pick the action which will give the most return!

2. Multiple Agents and an Actor-Critic approach:
Temporal learning works very well, but in our scenario, we don't only have one vehicle or agent, we have many! Even though each of these agents see different things and have different observations, the underlying logis the same for all, and so they should execute the same. But training them is where it gets tricky. It is possible that if one particular agent doesn't change its logic and reacts the same way, another agent may behave differently changing the end result. In the perspective of the first agent, it cannot account for that change in the end result as it didn't do anything differently! In order to solve this, we use a Actor-Critic approach, where the Actors are these agents executing the actions, and the Critic oversees everything! It knows what all the agents see, and what they all do, and can critisize each agent on where it's going wrong (more formally can calculate the loss of each agent). Thus the actors/agents can now again learn via temporal learning. It is easy to understand the Actor-Critic relationship more like that of a Player-Coach or Student-Teacher relationship.
